---
title: CTM读文笔记
weight: -105
draft: true
description: 一个新型的人脑启发式AI架构
slug: ctm-note
tags:
  - 论文
  - 笔记
  - 架构
series:
  - 论文笔记
series_order: 2
date: 2025-05-14
lastmod: 2025-05-20
authors:
  - Morethan
---
{{< katex >}}
{{< lead >}}
一个新型的人脑启发式架构，在现有时序无关的神经元模型的基础上引入了"时序"概念，涌现出一系列类似人脑的现象
{{< /lead >}}

## 前言

关注到这篇论文其实就是因为其宣称的那种"奇妙"的涌现现象，将时序信息引入神经元模型之后产生了**类似人类思维模式**的注意力现象。这些现象并非人为设计的，而是在训练过程中自然产生的，这是最令人振奋的。

最直观的例子就是官方给出的"迷宫模型"，从可视化的结果可以直观地看到，模型的注意力焦点在迷宫中游走，就像人类求解迷宫问题一样。另外官方还给出了一个可交互的展示网页：[Demo](https://pub.sakana.ai/ctm/)，在这个网页中你可以直接指挥模型求解迷宫，并实时查看模型的注意力焦点。

另外，官方的[代码仓库](https://github.com/SakanaAI/continuous-thought-machines)中配置了详尽的说明文档和代码注释，代码文件的切分也非常简洁直观，有一种程序员才懂的"艺术感"。


{{< alert icon="pencil" cardColor="#1E3A8A" textColor="#E0E7FF" >}}
在阅读了大量的屎山代码之后，可能才会对代码的"优雅"程度有所体会😇并深刻理解这份"优雅"背后的工作量
{{< /alert >}}

## 引入

神经网络最初受生物大脑启发，却与生物大脑差异巨大。生物大脑展现出随时间演变的复杂神经动力学过程，而现代神经网络为了便于大规模深度学习，可以摒弃了"时序"特征。

关于为什么要开展这项研究，官方论文中已经有了非常明确的表述，无需多言：

"为何开展此项研究？诚然，现代人工智能在诸多实践领域展现出的卓越性能似乎表明，对神经动力学的模拟实无必要，抑或显式考量智能的时间维度实属反实用之举。然而，人类智能具有高度灵活性、数据高效性以及优异的未见情境外推能力，且存在于学习与适应皆与时间之箭紧密关联的开放世界中。因此，人类智能天然具备常识运用、本体论推理能力、透明性/可解释性以及强大的泛化能力——这些特质在现有人工智能中尚未得到令人信服的体现。"

这篇文章的核心技术贡献如下所示：

1. 解耦的内部时间维度
2. 神经元层面的模型(NLMs)
3. 神经活动同步

当然，在没有详细看代码实现的情况下，光看这些名词是没有意义的。但是作者的思路和观点还是可以了解：

1. 推理模型和循环：作者锐评了当前推理模型，指出"继续扩展当前模型架构"这条技术路线已经被很多研究质疑。而使用循环技术也的确能够让现有模型架构产生更加良好的表现，但是作者认为，**循环机制很重要，但是被循环机制解锁的神经元活动之间的精确时序与交互同样重要**
2. 有趣的副作用：CTM 内部的循环类似于人类的思考，在没有任何显式的监督函数引导的情况下就能够**为不同难度的任务自动分配合适的计算资源**，简单的任务将会提前终止计算，而复杂的任务将会进行更加深入的计算
3. **信息可以被编码在时序动态中**，这将赋予网络更加强大的信息压缩能力


{{< alert icon="pencil" cardColor="#1E3A8A" textColor="#E0E7FF" >}}
说明一下我对第三点的思考，按照"压缩即智能"的观点，将将信息编码到时序动态中将极大地提高网络的信息"压缩率"，从某种程度上来说也就提升了"智能"
{{< /alert >}}

最后作者也明确了自己这个研究的目的所在：

"通过CTM显式建模神经时序机制，我们旨在为开发更具生物合理性且高性能的人工智能系统开辟新路径。"

## 方法

![CTM.png](img/CTM.png)

CTM架构概述：①突触模型（权重以蓝线表示）通过模拟神经元间相互作用生成预激活值。每个神经元会保留 ②预激活历史记录，其中最新数据被 ③神经元级模型（权重以红线表示）用于产生 ④后激活值。系统同时维护 ⑤后激活历史记录，并据此计算⑥同步矩阵。基于该矩阵 ⑦筛选神经元对，由此产生的 ⑧潜表征被CTM用于 ⑨生成输出并通过交叉注意力机制调节数据。经调节的数据（如注意力输出）会 ⑩与后激活值拼接，作为下一内部时钟步的输入

如果只是单纯的引入时间维度的话，传统的 RNN 架构也能够实现，但 CTM 的创新之处在于：

1. 使用**神经元级模型**取代了传统的激活函数
2. 使用**神经同步**作为潜在表征来调制数据并生成输出

### 连续思考：内部序列维度

论文定义了一个内部的**时间维度**：
$$
t\\in \\{1,\\dots,T\\}
$$

图中参数右上角的角标就指代的是某一个特定的时间步，而每一个时间步中都会进行一次图中完整的计算流程(从①到⑩)

这种内部维度并非全新的概念，RNN 和 Transformer 中都有使用。但是传统的架构按照数据输入的顺序来逐步处理，这**隐含**地将内部的时间维度与数据的输入顺序挂钩了。

举个例子，RNN 中也有内部时间维度的概念，只是每一个时间步模型都会接受一个新的 token 输入，然后根据内部隐状态去产生下一个 token 并更新内部隐状态。如果只看 token 的输入就会发现我们其实是默认将数据内在的顺序作为模型的内部时间维度


{{< alert icon="pencil" cardColor="#1E3A8A" textColor="#E0E7FF" >}}
Transformer 中其实并没有非常明显的顺序处理：每一个时间步注意力机制都会并行处理原来所有的 token🤔或许这也是其强大所在？
{{< /alert >}}

而 CTM 则完全解耦了这种关联，让内部的处理与输入数据无关。不只是顺序无关，还与输入序列的长度无关。这种"解耦"使得模型内部的"思考"可以有任意长度，可以迭代地构建和完善内部表征，并且可以扩展到"非序列化"的任务上，例如求解迷宫问题。

### 循环权重：突触模型

这一部分主要讲解**突触模型**，对应图中的番号①部分。形式化来讲突触模型进行的工作如下：
$$
a^t=f_{\\theta_{syn}}(concat(z^t,o^t))\\in R^D
$$
其中 \\(z^t\\) 表示 \\(t\\) 时间步的输入序列，\\(o^t\\) 是上一个时间步计算得出的**调制数据**，与原来的 \\(z^t\\) 进行拼接后整体送入**突触模型**进行计算，然后就得出 \\(a^t\\) 也就是**预激活值**

突触模型本质上就是一个函数 \\(f_{\\theta_{syn}}\\) 可以用多种方式表达，经过实验，原论文中使用了 MLP 来表示这个函数，更具体的就是 U-NET-esque MLP

然后将最近的 \\(M\\) 个预激活值用一个矩阵 \\(A^t\\) 存储起来：

$$
A^t=[a^{t-M+1}\\quad a^{t-M+1}\\dots a^{t}]\\in R^{D\\times M}
$$
历史序列中的前 \\(M\\) 个元素及初始时刻 \\(t=1\\) 的 \\(z\\) 值需要进行初始化，实验表明将其设置为可学习参数能获得最佳效果。

### 参数私有化的神经元级模型


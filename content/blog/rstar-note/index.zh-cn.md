---
title: rStar读文笔记
weight: -85
draft: false
description: 一个先进的小模型推理能力自提升框架
slug: rstar-note
tags:
  - 论文
  - 笔记
  - MCTS
  - 推理增强
series:
  - 论文笔记
series_order: 1
date: 2025-03-20
lastmod: 2025-03-20
authors:
  - Morethan
---
{{< katex >}}
{{< lead >}}
关键词：互推理；小模型；推理能力提升
{{< /lead >}}

## 研究背景

### 现有工作

以下内容由 AI 生成，快速解释 RAP 是什么。

研究基础是一种名为 RAP（Reasoning via Planning）的框架，旨在通过将语言模型（LLM）同时作为世界模型和推理代理，并结合蒙特卡洛树搜索（MCTS）算法进行策略性探索，从而提升模型在复杂任务中的推理能力。

1. **问题背景**：
   - 当前的大型语言模型（LLM）在推理任务中存在局限性，主要原因是缺乏对环境的心理表征（世界模型），无法有效预测动作的结果和长期影响。
   - 此外，LLM 缺乏奖励机制来评估推理路径，也无法平衡探索和利用，导致推理效率低下。

2. **RAP 框架**：
   - **语言模型作为世界模型**：通过自然语言定义状态和动作，将推理过程建模为马尔可夫决策过程（MDP），并利用 LLM 预测每个动作的结果。
   - **奖励设计**：使用动作的对数概率、置信度以及 LLM 自身的评估结果作为奖励，引导推理走向理想状态。
   - **蒙特卡洛树搜索（MCTS）**：通过 MCTS 迭代构建搜索树，平衡探索和利用，最终选择高奖励的推理路径。

### 不足之处

1. LLM 难以有效探索解空间，常常会产生低质量的推理路径
2. LLM 难以准确评估推理路径质量的高低
3. 上述问题在小模型中更加突出

## 方法概述

### 拟人推理

推理路径仍然依赖 MCTS 来生成，但是提供了更丰富的模拟人类思考推理流程的动作：分解和搜索某一个推理步、提出子问题、问题转化等等

### 路径评估

使用相互一致性原则判定路径质量，即引入另外一个能力相当的小模型来作为“同伴”，以此判定推理路径的质量，具体流程如下：

1. 框架会给予“同伴”小模型一些局部推理路径作为提示，然后让这个小模型补全推理路径
2. 如果 MCTS 生成的路径与“同伴”小模型补全的路径一致，那么就认为这个推理路径质量高，可以作为候选路径


{{< alert icon="pencil" cardColor="#1E3A8A" textColor="#E0E7FF" >}}
使用能力相当的小模型，避免对大模型进行蒸馏；使用“同伴模型”对路径进行评估而非直接指导路径生成；判断"路径一致"主要依赖最终结果
{{< /alert >}}

## 具体方法论

符号说明表格：

| 符号             | 含义                  |
| -------------- | ------------------- |
| \(x\)            | 目标问题                |
| \(M\)            | 目标小模型               |
| \(T\)            | 小模型使用 MCTS 生成的搜索树   |
| \(s\)            | 推理中间步               |
| \(t\)            | 候选路径，\(T\) 中的一条完整推理路径 |
| \(ans\)          | \(M\) 解决 \(x\) 的最终推理路径  |
| \(Score\)        | 推理路径评价函数            |
| \(a\)            | 从动作空间中采样得到的一个动作     |
| \(s_{d}\)        | 终止推理步，包含问题的答案       |
| \(\hat{M}\)      | "同伴"小模型             |
| \(T_{validate}\) | 经过路径评估函数剪枝后的 \(T\)    |
| \(Estimate\)     | 路径评估函数              |

### 问题规范化

把“小模型解决推理问题”这个抽象的自然语言描述的过程形式化：

$$
t=x\oplus s_1 \oplus s_2 \oplus ...\oplus s_d
$$

$$
T=\left \{ t^1, t^2, ..., t^n \right \}
$$

$$
T_{validate}=Estimate(T)
$$

$$
ans = max(Score(T_{validate}))
$$

### 拟人推理

#### 动作空间

1. **一步思考**：给予现有推理路径，让模型产生下一步的推理
2. **快速思考**：直接让模型补全所有的推理步，直到产生最终结果
3. **子问题+回答**：让模型提出一个子问题并回答这个子问题
4. **子问题重回答**：上一步中产生的答案可能不准确，因此提供一个额外的动作选项，重新回答子问题
5. **问题重述**：让模型重新整理问题中的条件


{{< alert icon="triangle-exclamation" cardColor="#ffcc00" textColor="#333333" iconColor="#8B6914" >}}
注意，动作4 只能发生在动作3 之后，动作5 只能发生在问题本身(根节点)之后
{{< /alert >}}

#### 奖励函数

借鉴了 AlphaGo，将中间步的评价(奖励)设置为其对于正确的结果的贡献，具体实现如下：

1. 初始化 \(Q(s_{i},a_{i})=0\)；随机产生下一步，直到遇到终止结点 \(s_{d}\)
2. 使用**一致性投票**计算 \(Q(s_{d},a_{d})\)，也是终止结点的**置信度分数**
3. 反向传播：\(Q(s_{i},a_{i})=Q(s_{i},a_{i})+Q(s_{d},a_{d})\)

#### MCTS

大体上使用了经典的 MCTS：选择、扩展、模拟(Rollout)和反向传播；只是为了获取更加精确的奖励值，执行了多次模拟评估

探索-开发平衡依然使用经典 UCT 公式：

$$
UCT(s,a) = \frac{Q(s,a)}{N(s,a)}+c\sqrt{ \frac{\ln N_{parent}(s)}{N(s,a)} }
$$

其中 \(N(s,a)\) 表示某个节点被访问过的次数，\(Q(s,a)\) 就是可以被累计更新的奖励值，\(c\) 表示平衡率

### 路径评估

1. 对于一条推理路径 \(t=x\oplus s_1 \oplus s_2 \oplus ...\oplus s_d\)，随机抽取一个推理步 \(s_{i}\)
2. 将 \(s_{i}\) 之前的推理路径 \(t_{1}=x\oplus s_1 \oplus s_2 \oplus ...\oplus s_i\) 作为提示词注入"同伴"小模型 \(\hat{M}\)
3. \(\hat{M}\) 补全路径，产生新路径 \(t'=x\oplus s_1 \oplus s_2 \oplus ...\oplus s_i \oplus s_{i+1}' \oplus \dots \oplus s_{d}'\)
4. 如果"路径一致"也就是得出的问题答案一致，那么 \(t'\) 就被视作候选路径


{{< alert icon="pencil" cardColor="#1E3A8A" textColor="#E0E7FF" >}}
选出候选路径的过程就是 \(T\) 被剪枝的过程，最终产生 \(T_{validate}\)
{{< /alert >}}

### 最终选择

对于候选路径树中的每一个候选路径 \(t=x\oplus s_1 \oplus s_2 \oplus ...\oplus s_d\)

$$
Score(t)=\prod_{i=1}^{d} Q(s_{i},a)
$$

最后选择分数最大的那一条推理路径：

$$
ans = max(Score(T_{validate}))
$$

## 其他细节

1. MCTS 执行 32 次 Rollout
2. 处理 MATH 数据集的时候 MCTS 最大深度为 8，其余情况最大深度为 5
3. "同伴"小模型的推理与目标小模型的推理可以并行执行，提高计算效率
4. 路径评估的时候，路径截断点应该处于总路径的 \(20\% \sim 80\%\) 之间

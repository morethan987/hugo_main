---
title: 大模型基础读书笔记
weight: -110
draft: true
description: 阅读浙大的教材大模型基础后写的读书笔记
slug: llm-foundation-notes
language: zh-cn
tags:
  - LLM
  - 工程实践
  - 笔记
series:
  - AI工程
series_order: 2
date: 2025-05-28
lastmod: 2025-05-28
authors:
  - Morethan
---
{{< lead >}}
阅读浙大的教材大模型基础后写的读书笔记
{{< /lead >}}

## 前言

现在的大模型技术发展可谓是"日新月异"，好像每天都有令人震撼的重要进展🤔

事实上，这些"重要进展"到底有多有效？工程实践上的价值到底有多少？这些问题最终还是要回到那些"平凡"的经典知识上。

关注大模型技术最新的进展当然是好事：这有助于我们了解研究方向，开拓思维。但是不能只关注那些不断发展的顶层技术，还要关注那些发展背后的、底层的、不变的东西。

这也是在这个技术突飞猛进的时代，沉下心去阅读教材的意义所在。

本篇主要关注浙大编写的 [《大模型基础》](https://github.com/ZJU-LLMs/Foundations-of-LLMs)一书，这本书目前还在不断更新中。

## 语言模型基础

这部分内容就是目前大学中最常教授的知识内容：基于 n-grams 的统计学语言建模，基于 RNN 的语言模型和基于 Transformer 的语言模型。下面贴一张 Transformer 架构的流程图。

![Transformer](Transformer.png "引用自第 16 页")

模型架构没什么好说的，但这里有几个比较有意思的问题，在我看书之前还比较模糊，看书之后就立刻清晰了。

---

Q：都说 Transformer 是并行计算的架构，但为什么还是只能一个 token 接着一个 token 串行生成？

A：Transformer 的并行计算体现在处理已知序列上，所有已知的 token 的向量会被并行地送入模型中进行处理。Token 的生成过程还是串行地自回归地生成。

---

Q：将残差连接和层归一化的顺序调换对模型有影响吗？

A：将层归一化放在残差连接后面的网络叫做 Post-LN，还有一种将层归一化放在残差连接之前的网络叫做 Pre-LN；Post-LN 应对表征坍塌的能力更强，处理梯度消失的能力略弱，Pre-LN 反之。

Q：表征坍塌是什么意思？

A：在训练过程中，模型学习到的表示变得缺乏区分性，也就是说不同的输入样本被映射到相似甚至相同的特征向量，导致模型失去了对于不同样本的判别性。

---

Q：为什么 Transformer 的 Encoder 部分和 Decode 部分可以单独构建语言模型？

A：

---



## 大语言模型架构

## 参数高效微调

## 模型编辑
